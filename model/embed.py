# -*- coding: utf-8 -*-
"""embed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lS5Zpuw4IAUP4i40ID9PBRH0B6SbIlBu
"""

# Pinecone 적용과 RAG 파이프라인 구축

from dotenv import load_dotenv
import os
from langchain_community.vectorstores import Pinecone as LangchainPinecone
from langchain_upstage import UpstageDocumentParseLoader, UpstageEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_upstage import ChatUpstage
from langchain.chains import LLMChain
from langchain.prompts.chat import ChatPromptTemplate
from langchain.output_parsers import RetryOutputParser
from pinecone import Pinecone, ServerlessSpec

# 1. 환경 변수 로드
load_dotenv()

# Pinecone 클라이언트 설정
pc = Pinecone(
    api_key=os.getenv("PINECONE_API_KEY"),
)

# Pinecone 인덱스 생성
index_name = "ssafy-index"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=4096,  # 임베딩 벡터 크기
        metric="cosine", # 코사인 유사도를 기준으로 검색이 가능한 인덱스 설정
        spec=ServerlessSpec(
            cloud="aws",
            region=os.getenv("PINECONE_ENVIRONMENT"),
        ),
    )

# pc.delete_index(index_name)
## 임베딩딩 모델 출력 차원에 맞게 설정할려고 지움.
## 임베딩 모델의 출력 차원이 4096, 따라서, 처음에 설정했던 1024에서 4096로 바꿈꿈

# 2. 데이터 수집 및 문서 로드
directory_path = r"C:\Users\SSAFY\Desktop\RAG\RAG_AI\model\data"
file_paths = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.pdf')]

print(f"Files to process: {file_paths}")
# pdf 문서 로드 및 파싱 : UpstageDocumentParseLoader 사용하여 pdf -> html 형태로 파싱
loaded_documents = []
for file_path in file_paths:
    try:
        loader = UpstageDocumentParseLoader(file_path,
                                            output_format='html',
                                            coordinates=False)
        document = loader.load()
        # 파일명 metatdata에 추가
        for page in document:
            page.metadata["file_name"] = os.path.basename(file_path)
        loaded_documents.append(document)
        print(f"Loaded document: {file_path}")
    except Exception as e:
        print(f"Error loading {file_path}: {e}")

# 3. 텍스트 분할 및 전처리 : RecursiveCharacterTextSplitter 활용하여 텍스트 지정된 크기(1000자)로 분할, 중복 처리
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
texts = []
for doc in loaded_documents:
    texts.extend(text_splitter.split_documents(doc))

print(f"전체 청크 개수: {len(texts)}")
print(texts[1].metadata)

# 4. 임베딩 생성 및 벡터 DB 구축 : UpstageEmbeddings 사용하여 텍스트(1000자) -> 벡터 형식
embeddings = UpstageEmbeddings(model="solar-embedding-1-large")

# 텍스트와 메타데이터, ID 매핑
texts_to_store = [text.page_content for text in texts]
metadatas = [text.metadata for text in texts]
ids = [f"text-{i}" for i in range(len(texts))]  # 고유 ID 생성

# 벡터 DB 생성 및 데이터 추가 : Pinecone vector Store을 사용하여 임베딩을 인덱스에 업로드
vectorstore = LangchainPinecone.from_texts(
    texts=texts_to_store,
    embedding=embeddings,
    metadatas=metadatas,
    ids=ids,
    index_name=index_name
)